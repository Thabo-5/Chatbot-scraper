{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries which will assist in removing the html tags\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "\"\"\"Remove html tags from messages\"\"\"\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 1 explore_.txt\n",
      "Complete 2 explore_about-us_our-courses.txt\n",
      "Complete 3 explore_about-us_our-values.txt\n",
      "Complete 4 explore_alumni_hire-explorer.txt\n",
      "Complete 5 explore_alumni_student-reviews.txt\n",
      "Complete 6 explore_contact.txt\n",
      "Complete 7 explore_course_info_1_3.txt\n",
      "Complete 8 explore_course_info_1_3_1.txt\n",
      "Complete 9 explore_course_info_1_3_oncampus.txt\n",
      "Complete 10 explore_course_info_1_3_online.txt\n",
      "Complete 11 explore_course_info_2_4.txt\n",
      "Complete 12 explore_course_info_34_3.txt\n",
      "Complete 13 explore_course_info_34_3_1.txt\n",
      "Complete 14 explore_course_info_34_3_online.txt\n",
      "Complete 15 explore_course_info_35_4.txt\n",
      "Complete 16 explore_course_info_38_5.txt\n",
      "Complete 17 explore_course_info_38_5_online.txt\n",
      "Complete 18 explore_course_info_39_5.txt\n",
      "Complete 19 explore_course_info_39_5_online.txt\n",
      "Complete 20 explore_course_info_3_3.txt\n",
      "Complete 21 explore_course_info_3_3_1.txt\n",
      "Complete 22 explore_course_info_3_3_online.txt\n",
      "Complete 23 explore_course_info_40_5.txt\n",
      "Complete 24 explore_course_info_40_5_online.txt\n",
      "Complete 25 explore_course_info_41_5.txt\n",
      "Complete 26 explore_course_info_41_5_online.txt\n",
      "Complete 27 explore_course_info_42_5.txt\n",
      "Complete 28 explore_course_info_42_5_online.txt\n",
      "Complete 29 explore_course_info_43_5.txt\n",
      "Complete 30 explore_course_info_43_5_online.txt\n",
      "Complete 31 explore_course_info_4_3.txt\n",
      "Complete 32 explore_course_info_4_3_1.txt\n",
      "Complete 33 explore_course_info_4_3_online.txt\n",
      "Complete 34 explore_course_info_53_5_online.txt\n",
      "Complete 35 explore_course_info_8_4.txt\n",
      "Complete 36 explore_enterprise_build-pipeline.txt\n",
      "Complete 37 explore_enterprise_summary.txt\n",
      "Complete 38 explore_login.txt\n",
      "Complete 39 explore_password.txt\n",
      "Complete 40 explore_signup.txt\n"
     ]
    }
   ],
   "source": [
    "directory = 'txt_files/Raw_v2'\n",
    "faq_dict = {}\n",
    "counter = 0\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if (filename.endswith(\".txt\") and 'blog_' not in filename):\n",
    "        f = open('txt_files/Raw_v2/{}'.format(filename), 'r+', encoding='utf-8')\n",
    "        string_list = f.readlines()\n",
    "        \n",
    "        if len(string_list) > 0:\n",
    "            \n",
    "            # Cleans blank lines and random sub-headings from the text (Part 1 of 2)\n",
    "            new_lines = [line for line in string_list if line != '\\n' and line != ' \\n']\n",
    "            new_lines = [strip_tags(line) for line in new_lines]\n",
    "            new_lines = [re.sub(' +', ' ', line) for line in new_lines]\n",
    "            new_lines = [re.sub('\\xa0', '', line) for line in new_lines]\n",
    "            for i in range(len(new_lines) - 1):\n",
    "                if new_lines[i] == new_lines[i+1]:\n",
    "                    new_lines[i] = ' '\n",
    "            for i in range(len(new_lines)):\n",
    "                if new_lines[i].startswith(' '):\n",
    "                    new_lines[i] = ' '\n",
    "            new_lines = [line for line in new_lines if line != ' ']\n",
    "\n",
    "            try:\n",
    "                # Look for questions that aren't in headings\n",
    "                for i in range(len(new_lines)):\n",
    "                    if '?' in new_lines[i]:\n",
    "                        if new_lines[i] in faq_dict.keys():\n",
    "                            a = faq_dict[new_lines[i]]\n",
    "                            if new_lines[i + 1] not in a:\n",
    "                                a.append(new_lines[i + 1])\n",
    "                                faq_dict[new_lines[i]] = a\n",
    "                        else:\n",
    "                            faq_dict[new_lines[i]] = [new_lines[i + 1]]\n",
    "\n",
    "                # Look for questions in H2 headings\n",
    "                    if ('?' in new_lines[i] and 'H2' in new_lines[i]):\n",
    "                        key = new_lines[i]\n",
    "                        vals = []\n",
    "                        j = i + 1\n",
    "                        while 'H2' not in new_lines[j]:\n",
    "                            vals.append(new_lines[j])\n",
    "                            j += 1\n",
    "                        if key in faq_dict.keys():\n",
    "                            a = faq_dict[key]\n",
    "                            for k in range(len(vals)):\n",
    "                                if vals[k] not in a:\n",
    "                                    a.append(vals[k])\n",
    "                                    faq_dict[key] = a\n",
    "                        else:\n",
    "                            faq_dict[key] = vals\n",
    "\n",
    "                # Look for questions in H3 headings\n",
    "                    if ('?' in new_lines[i] and 'H3' in new_lines[i]):\n",
    "                        key = new_lines[i]\n",
    "                        vals = []\n",
    "                        j = i + 1\n",
    "                        while 'H2' not in new_lines[j]:\n",
    "                            vals.append(new_lines[j])\n",
    "                            j += 1\n",
    "                        if key in faq_dict.keys():\n",
    "                            a = faq_dict[key]\n",
    "                            for k in range(len(vals)):\n",
    "                                if vals[k] not in a:\n",
    "                                    a.append(vals[k])\n",
    "                                    faq_dict[key] = a\n",
    "                        else:\n",
    "                            faq_dict[key] = vals\n",
    "            except:\n",
    "                print('Skipped file: {}'.format(filename))\n",
    "            \n",
    "            # Cleans blank lines and random sub-headings from the text (Part 2 of 2)\n",
    "            new_lines = [line for line in new_lines if line not in faq_dict.keys() and line not in faq_dict.values() and 'FAQ' not in line]\n",
    "            new_lines = [re.sub('H\\d: ', '', line) for line in new_lines]\n",
    "            new_lines = [re.sub('Paragraph: ', '', line) for line in new_lines]\n",
    "            new_lines = [line for line in new_lines if line != '\\n' and line != ' \\n']\n",
    "            final_lines = []\n",
    "            final_lines = [line for line in new_lines if line not in final_lines]\n",
    "            final_lines = list(dict.fromkeys(final_lines))\n",
    "\n",
    "            # Write cleaned text data to new text file\n",
    "            if len(final_lines) > 0:\n",
    "                f_cl = open('txt_files/Cleaned_v2/cleaned_{}'.format(filename), 'w+', encoding='utf-8')\n",
    "                for i in range(len(final_lines)):\n",
    "                    f_cl.write(final_lines[i])\n",
    "                f_cl.close()\n",
    "        f.close()\n",
    "    counter += 1\n",
    "    print('Complete {0} {1}'.format(counter, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://explore-datascience.net/signup\\n',\n",
       " 'Welcome To Our Universe!\\n',\n",
       " 'CREATE MY ACCOUNT\\n',\n",
       " 'Ok\\n',\n",
       " 'ok\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Talented, eager and hampered by access to funding? Apply for a loan to finance your education. EXPLORE aims to provide affordable rates for students who meet our criteria.\n",
      "\n",
      "['Why EXPLORE?\\n', 'Our Payment Plans\\n', 'Upfront Payment\\n', 'EXPLORE Student Loan\\n']\n",
      "\n",
      "\n",
      "Why EXPLORE?\n",
      "\n",
      "['We pride ourselves on our innovative, practical and real-world-readiness approach to teaching and learning. Our scientists, who facilitate all our programmes, have experience solving difficult problems across a variety of industries in South Africa and abroad.\\n']\n",
      "\n",
      "\n",
      "EXPLORE is Africa’s largest Data Science Academy - thousands of students have trusted us with their futures and not been disappointed - what are you waiting for? \n",
      "\n",
      "['Transform your workforce.\\n', 'Why EXPLORE?\\n']\n",
      "\n",
      "\n",
      "Why Us?\n",
      "\n",
      "['Enterprise\\n']\n",
      "\n",
      "\n",
      "What are you waiting for? \n",
      "\n",
      "['I came from a business focused role. I had a desire to break into tech, more specifically Data Science and EXPLORE’s Data Science program became the catalyst for that ambition to materialise. I went from knowing nothing about coding to now working in a role where I get to build and implement effective software solutions for some of the biggest organizations in South Africa. \\n']\n",
      "\n",
      "\n",
      "Which course are you most interested in?\n",
      "\n",
      "['Consider your passion, strengths and your dream job. Select one of the tiles below, then click Next.\\n']\n",
      "\n",
      "\n",
      "How much time do you want to spend learning a new skill?\n",
      "\n",
      "[\"Consider the amount of time you can invest and what commitment you'd like to make.\\n\"]\n",
      "\n",
      "\n",
      "Where do you want to study?\n",
      "\n",
      "['Which place suits you best?\\n']\n",
      "\n",
      "\n",
      "Which place suits you best?\n",
      "\n",
      "['Cape Town\\n']\n",
      "\n",
      "\n",
      "“Which of these will stretch, broaden, expand and challenge me?”\n",
      "\n",
      "['“Which is less comfortable, more painful and consumes more courage?”\\n']\n",
      "\n",
      "\n",
      "“Which is less comfortable, more painful and consumes more courage?”\n",
      "\n",
      "['Then choose that path.\\n']\n",
      "\n",
      "\n",
      "EXPLORE is proud to be offering a loan option whereby eligible students can apply to fund their studies at an affordable rate, paid off over a longer period. Candidates will have 36 months to pay. Innovative? We think so!\n",
      "\n",
      "['Scholarship\\n', 'Upfront Payment\\n']\n",
      "\n",
      "\n",
      "You're keen to learn what Data Science is all about and master some vital skills when it comes to data management, analysis and insights? Look no further, this course meets all those needs, and more!\n",
      "\n",
      "['Enroll now\\n']\n",
      "\n",
      "\n",
      "From \"What is Data Science?\" through to Prediction Models in the workplace, we've got all your questions covered. This high-level, comprehensive walk through the world of Data Science will leave you eager to learn more, and apply your knowledge in whatever it is that you do!\n",
      "\n",
      "['What is Data Science?\\n']\n",
      "\n",
      "\n",
      "What is Data Science?\n",
      "\n",
      "['Understand the history, misconceptions, roles, and potential of data science in society.\\n']\n",
      "\n",
      "\n",
      "Why Explore? \n",
      "\n",
      "['World Class Technology\\n']\n",
      "\n",
      "\n",
      "Don’t have an account? Register Now\n",
      "\n",
      "['Register Now\\n']\n",
      "\n",
      "\n",
      "Forgot Password?\n",
      "\n",
      "['ok\\n', 'Welcome!\\n', 'Register Now\\n']\n",
      "\n",
      "\n",
      "Forgot Your Password?\n",
      "\n",
      "[\"Fill in your email address below and we'll send you a link to\\n\"]\n",
      "\n",
      "\n",
      "Already have an account? Login\n",
      "\n",
      "['Ok\\n', 'Welcome To Our Universe!\\n']\n"
     ]
    }
   ],
   "source": [
    "for key in faq_dict.keys():\n",
    "    print('\\n')\n",
    "    print(key)\n",
    "    print(faq_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual checks\n",
    "# Deleted files:\n",
    "### 1. faq#Pricingcollapse3.txt and cleaned_faq#Pricingcollapse3.txt: No useful info\n",
    "### 2. faq.txt and cleaned_faq.txt: No useful info\n",
    "\n",
    "# Other info:\n",
    "### 1. Many of the text files have info that also appears in the faq_dict. This info was not deleted because\n",
    "###    it was recommended to leave as much informative text data in the files as possible.\n",
    "### 2. contact.txt required email addresses. These were manually added.\n",
    "### 3. Removed duplicated text from individual files. Some info is duplicated across files, this was not removed.\n",
    "### 4. cleaned_data-engineering.txt: email address required on line 118, could not find it on website.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining QA with annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./answers.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "questions = data['data'][0]['paragraphs'][0]['qas']\n",
    "ids = []\n",
    "\n",
    "for question in questions:\n",
    "    ids.append(question['id'])\n",
    "max_id = max(ids)\n",
    "extracted_faqs = []\n",
    "for key in range(len(faq_dict.keys())):\n",
    "    new_questions = {'question':str(),\n",
    "                'id': int(),\n",
    "                'answers':[{'text':str(),\n",
    "                          'answer_start': int(), # Problem: need to be able to get start token from files\n",
    "                          'answer_category': None}],\n",
    "                'is_impossible': False}\n",
    "    new_questions['question'] = list(faq_dict.keys())[key]\n",
    "    # Cleaning question\n",
    "    new_questions['question'] = re.sub(\"H1: \", \"\", new_questions['question'])\n",
    "    new_questions['question'] = re.sub(\"H2: \", \"\", new_questions['question'])\n",
    "    new_questions['question'] = re.sub(\"H3: \", \"\", new_questions['question'])\n",
    "    new_questions['question'] = re.sub(\"\\n\", \"\", new_questions['question'])\n",
    "\n",
    "    new_questions['id'] = max_id + key\n",
    "    new_questions['answers'][0]['text'] = \" \".join(faq_dict[list(faq_dict.keys())[key]])\n",
    "\n",
    "    # cleaning answers\n",
    "    new_questions['answers'][0]['text'] = re.sub(\"\\n\", \"\", new_questions['answers'][0]['text'])\n",
    "    new_questions['answers'][0]['text'] = re.sub(\"H1\", \"\", new_questions['answers'][0]['text'])\n",
    "    new_questions['answers'][0]['text'] = re.sub(\"H2\", \"\", new_questions['answers'][0]['text'])\n",
    "    new_questions['answers'][0]['text'] = re.sub(r\"H3: \", \"\", new_questions['answers'][0]['text'])\n",
    "    new_questions['answers'][0]['text'] = re.sub(\"Paragraph: \", \"\", new_questions['answers'][0]['text'])\n",
    "    new_questions['answers'][0]['text'] = re.sub(\"’\", \"'\", new_questions['answers'][0]['text'])\n",
    "\n",
    "    new_questions['answers'][0]['answer_start'] = None\n",
    "    new_questions['answers'][0]['answer_category'] = None\n",
    "    new_questions['is_impossible'] = False\n",
    "    extracted_faqs.append(new_questions)\n",
    "\n",
    "appended_questions =  questions + extracted_faqs\n",
    "data['data'][0]['paragraphs'][0]['qas'] = appended_questions\n",
    "with open('./training_data.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(data, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dict contains fields not in fieldnames: 'Country', 'No', 'Name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-68000eacc7d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteheader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdict_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I/O error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\csv.py\u001b[0m in \u001b[0;36mwriterow\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dict_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrowdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowdicts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\csv.py\u001b[0m in \u001b[0;36m_dict_to_list\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwrong_fields\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m                 raise ValueError(\"dict contains fields not in fieldnames: \"\n\u001b[1;32m--> 151\u001b[1;33m                                  + \", \".join([repr(x) for x in wrong_fields]))\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrowdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dict contains fields not in fieldnames: 'Country', 'No', 'Name'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "csv_columns = ['Answer']\n",
    "dict_data = [\n",
    "{'No': 1, 'Name': 'Alex', 'Country': 'India'},\n",
    "{'No': 2, 'Name': 'Ben', 'Country': 'USA'},\n",
    "{'No': 3, 'Name': 'Shri Ram', 'Country': 'India'},\n",
    "{'No': 4, 'Name': 'Smith', 'Country': 'USA'},\n",
    "{'No': 5, 'Name': 'Yuva Raj', 'Country': 'India'},\n",
    "]\n",
    "csv_file = \"Names.csv\"\n",
    "try:\n",
    "    with open(csv_file, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        for data in dict_data:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
